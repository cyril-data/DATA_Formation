{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T10:16:29.304678Z",
     "start_time": "2020-09-23T10:16:25.743057Z"
    }
   },
   "source": [
    "# Configuration et SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:14.374873Z",
     "start_time": "2020-09-30T13:14:14.372981Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init(\"/opt/spark\")\n",
    "\n",
    "# import pyspark.sql\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"S3ImagesRead\").getOrCreate()\n",
    "\n",
    "# data = [\n",
    "#     ('1990-05-03', 29, True),\n",
    "#     ('1994-09-23', 25, False)\n",
    "# ]\n",
    "# df = spark.createDataFrame(data, ['dob', 'age', 'is_fan'])\n",
    "# df.show()\n",
    "# import os\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "# import pyspark # only run after findspark.init()\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# data = [\n",
    "#     ('1990-05-03', 29, True),\n",
    "#     ('1994-09-23', 25, False)\n",
    "# ]\n",
    "# df = spark.createDataFrame(data, ['dob', 'age', 'is_fan'])\n",
    "# df.show()\n",
    "\n",
    "# df = spark.sql('''select 'spark' as hello ''')\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:14.377982Z",
     "start_time": "2020-09-30T13:14:14.376099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 02:25:08) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:18.175899Z",
     "start_time": "2020-09-30T13:14:14.378927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKIASE4SLVNMWHACNNOX zC2SJ4HR2lxZT1kektB8dwEWEA20TXWUBeZgAVbA\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "aws_profile = \"default\"\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "accessKeyId = config.get(aws_profile, \"aws_access_key_id\") \n",
    "secretAccessKey = config.get(aws_profile, \"aws_secret_access_key\")\n",
    "\n",
    "print(accessKeyId, secretAccessKey)\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2,databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark-shell' \n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=accessKeyId\n",
    "\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=secretAccessKey\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# memory = '10g'\n",
    "# pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "# https://stackoverflow.com/questions/37986963/set-drivers-memory-size-programmatically-in-pyspark\n",
    "\n",
    "# https://stackoverflow.com/questions/21138751/spark-java-lang-outofmemoryerror-java-heap-space\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"S3Images\").getOrCreate()\n",
    "\n",
    "# image_df = spark.read.format(\"binaryFile\").option(\"dropInvalid\", True).load(\"s3a://projet8/Simple-training/Dates/0_100.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T10:16:29.308198Z",
     "start_time": "2020-09-23T10:16:29.305886Z"
    }
   },
   "source": [
    "# Lecture images sur s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:20.301525Z",
     "start_time": "2020-09-30T13:14:18.177012Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "basePath = \"s3a://projet8usaouest/training/*\"\n",
    "\n",
    "image_df =  ImageSchema.readImages(basePath)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:20.332480Z",
     "start_time": "2020-09-30T13:14:20.302551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               image|\n",
      "+--------------------+\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "|[s3a://projet8usa...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform pyspark dataframe avec les label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:21.200988Z",
     "start_time": "2020-09-30T13:14:20.333419Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:23.215919Z",
     "start_time": "2020-09-30T13:14:21.203160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.77 ms, sys: 5.19 ms, total: 15 ms\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"ResNet50\")\n",
    "# lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
    "# p = Pipeline(stages=[featurizer, lr])\n",
    "\n",
    "model = featurizer.transform(image_df)    # train_images_df is a dataset of images and labels\n",
    "\n",
    "# model = p.fit(image_df_with_label)    # train_images_df is a dataset of images and labels\n",
    "\n",
    "# # Inspect training error\n",
    "# df = model.transform(train_images_df.limit(10)).select(\"image\", \"probability\",  \"uri\", \"label\")\n",
    "# predictionAndLabels = df.select(\"prediction\", \"label\")\n",
    "# evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "# print(\"Training set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               image|            features|\n",
      "+--------------------+--------------------+\n",
      "|[s3a://projet8usa...|[0.44355976581573...|\n",
      "|[s3a://projet8usa...|[0.22601045668125...|\n",
      "|[s3a://projet8usa...|[0.28762984275817...|\n",
      "|[s3a://projet8usa...|[0.28490749001502...|\n",
      "|[s3a://projet8usa...|[0.34204727411270...|\n",
      "|[s3a://projet8usa...|[0.28070768713951...|\n",
      "|[s3a://projet8usa...|[0.39465352892875...|\n",
      "|[s3a://projet8usa...|[0.22799128293991...|\n",
      "|[s3a://projet8usa...|[0.32174050807952...|\n",
      "|[s3a://projet8usa...|[0.92403715848922...|\n",
      "|[s3a://projet8usa...|[0.47011968493461...|\n",
      "|[s3a://projet8usa...|[0.57204377651214...|\n",
      "|[s3a://projet8usa...|[0.41625764966011...|\n",
      "|[s3a://projet8usa...|[0.82780987024307...|\n",
      "|[s3a://projet8usa...|[0.58401256799697...|\n",
      "|[s3a://projet8usa...|[0.91068315505981...|\n",
      "|[s3a://projet8usa...|[0.70584183931350...|\n",
      "|[s3a://projet8usa...|[0.54589545726776...|\n",
      "|[s3a://projet8usa...|[0.44996333122253...|\n",
      "|[s3a://projet8usa...|[0.40083500742912...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:33.082856Z",
     "start_time": "2020-09-30T13:14:23.217280Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 577 ms, sys: 60.9 ms, total: 638 ms\n",
      "Wall time: 5min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collect_m = model.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:14:33.087823Z",
     "start_time": "2020-09-30T13:14:33.084244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1963, 2)\n",
      "CPU times: user 68.3 ms, sys: 268 µs, total: 68.6 ms\n",
      "Wall time: 67.1 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/env/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(np.shape(collect_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T12:15:03.227595Z",
     "start_time": "2020-09-22T12:15:03.222803Z"
    }
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:16.710156Z",
     "start_time": "2020-09-30T13:14:33.089254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 19.9 ms, total: 154 ms\n",
      "Wall time: 21min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "pca = PCA(k=100, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model_pca = pca.fit(model.select(\"features\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:16.738160Z",
     "start_time": "2020-09-30T13:15:16.711178Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model_pca.transform(model.select(\"features\")).select(\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:23.209898Z",
     "start_time": "2020-09-30T13:15:16.739353Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features = result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:28.828156Z",
     "start_time": "2020-09-30T13:15:23.210800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         pcaFeatures|\n",
      "+--------------------+\n",
      "|[-9.7006733052283...|\n",
      "|[-10.175501926536...|\n",
      "|[-10.033015599898...|\n",
      "|[-9.9126685182653...|\n",
      "|[-9.8480816923279...|\n",
      "|[-9.9048304184214...|\n",
      "|[-9.0890003630531...|\n",
      "|[-9.9725009004847...|\n",
      "|[-9.9047117145424...|\n",
      "|[19.4575887295206...|\n",
      "|[18.2486066799351...|\n",
      "|[18.1938363314288...|\n",
      "|[18.8252097502555...|\n",
      "|[19.6370065237052...|\n",
      "|[18.7857715193263...|\n",
      "|[18.8273438104998...|\n",
      "|[18.8782233904406...|\n",
      "|[18.4022001124634...|\n",
      "|[18.3504665609669...|\n",
      "|[17.2987845820431...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:28.831510Z",
     "start_time": "2020-09-30T13:15:28.829177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1963, 1, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:33.157450Z",
     "start_time": "2020-09-30T13:15:28.832494Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "collect_path = image_df.select(\"image.origin\").collect()\n",
    "# collect_data = image_df.select(\"image\").collect()\n",
    "\n",
    "path_label_feature = []\n",
    "\n",
    "# Si une seule classe : \n",
    "for i, path in enumerate(collect_path) : \n",
    "    path_label_feature.append((path.origin, path.origin.split(\"/\")[3], features[i].pcaFeatures)  )\n",
    "    \n",
    "    \n",
    "# # Si plusieurs classes : \n",
    "# for i, path in enumerate(collect_path) : \n",
    "#     path_label_feature.append((path.origin, path.origin.split(\"/\")[3], features[i].pcaFeatures)  )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:33.161919Z",
     "start_time": "2020-09-30T13:15:33.158411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3a://projet8usaouest/training/Cherry Wax Yellow/0_100.jpg',\n",
       " 'training',\n",
       " DenseVector([-9.7007, 0.6393, 0.7278, 2.4237, 5.2484, 6.858, -3.749, 4.8533, -1.545, 1.3488, 2.5833, -12.3329, -0.1188, -2.1576, 2.2259, 5.8889, 2.9137, 2.1516, 5.3931, -6.2851, -4.7238, 3.1626, 11.2566, -3.0269, -0.692, 3.8179, -6.8806, 1.7268, 4.7458, 1.0889, -0.632, -7.7454, -1.915, -1.3238, -2.2568, -4.7081, -2.071, -1.0394, -5.9574, -0.2136, -2.846, 0.7611, 0.085, -3.9416, -0.4491, -0.9567, -1.031, 3.6803, 1.9215, -0.8564, 1.1307, -0.807, -1.9376, -0.7231, -0.5194, 0.8559, -0.8659, -1.8808, 1.1782, -1.7685, 0.5259, 3.6539, -0.78, -0.2811, -0.1066, 0.1227, 2.3517, 0.8531, -2.6377, -1.3026, 2.8584, 1.5626, -0.9428, 0.0049, 1.822, 1.2438, 0.3639, 0.2397, -0.0004, -1.5983, -2.1225, -0.0143, -2.3881, -1.7228, 0.544, 0.3143, -0.1251, 0.293, 2.5303, 4.5559, -1.0404, -0.3703, 0.6066, 0.4869, -2.109, -1.3898, -1.4725, -1.9471, 1.0338, 0.957]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "path_label_feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:33.197244Z",
     "start_time": "2020-09-30T13:15:33.163063Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "path_label_feature_df = spark.createDataFrame(path_label_feature, [\"path\", \"label\", 'features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T13:15:33.589915Z",
     "start_time": "2020-09-30T13:15:33.198117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                path|\n",
      "+--------------------+\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "|s3a://projet8usao...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_label_feature_df.select(\"path\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T13:14:14.393Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "writePath = \"s3a://projet8usaouest/features\"\n",
    "path_label_feature_df.write.parquet(writePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+\n",
      "|                path|   label|            features|\n",
      "+--------------------+--------+--------------------+\n",
      "|s3a://projet8usao...|training|[-9.7006733052283...|\n",
      "|s3a://projet8usao...|training|[-10.175501926536...|\n",
      "|s3a://projet8usao...|training|[-10.033015599898...|\n",
      "|s3a://projet8usao...|training|[-9.9126685182653...|\n",
      "|s3a://projet8usao...|training|[-9.8480816923279...|\n",
      "|s3a://projet8usao...|training|[-9.9048304184214...|\n",
      "|s3a://projet8usao...|training|[-9.0890003630531...|\n",
      "|s3a://projet8usao...|training|[-9.9725009004847...|\n",
      "|s3a://projet8usao...|training|[-9.9047117145424...|\n",
      "|s3a://projet8usao...|training|[19.4575887295206...|\n",
      "|s3a://projet8usao...|training|[18.2486066799351...|\n",
      "|s3a://projet8usao...|training|[18.1938363314288...|\n",
      "|s3a://projet8usao...|training|[18.8252097502555...|\n",
      "|s3a://projet8usao...|training|[19.6370065237052...|\n",
      "|s3a://projet8usao...|training|[18.7857715193263...|\n",
      "|s3a://projet8usao...|training|[18.8273438104998...|\n",
      "|s3a://projet8usao...|training|[18.8782233904406...|\n",
      "|s3a://projet8usao...|training|[18.4022001124634...|\n",
      "|s3a://projet8usao...|training|[18.3504665609669...|\n",
      "|s3a://projet8usao...|training|[17.2987845820431...|\n",
      "+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_label_feature_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
